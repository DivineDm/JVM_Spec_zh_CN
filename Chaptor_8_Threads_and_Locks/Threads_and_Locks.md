# 第八章 线程和锁

本章介绍一些底层的基本操作(action)，这些操作将让我们能更好的看清Java虚拟机和主存是如何交互的。

## 8.1 术语和框架定义

变量在程序中随处可见，变量不仅包含类变量(class本身也是一种变量)和实例变量(class对象实例变量)，也包含集合中的元素等变量。变量被存储在主存(main memory)中，并被进程中的所有线程共享。对于一个线程T，他拥有的参数和变量，别的线程是无法引用到的，因此我们不用关心T拥有的参数和变量是保存在主存中，还是线程栈(working memory)中。

>**Note:** 这里对于线程参数，我理解原文想表达的是这个参数从引用关系来讲，只从属于T。因此对于主线程将一个变量v同时通过构造函数传递给T1和T2，从而形成T1和T2互相影响v的场景不是作者想表达的场景

每一个线程都有自己的线程栈，线程栈中保存线程执行use和assign操作(action)需要用到的程序变量副本，该副本是从主存中复制而来的。当一个线程执行一个程序时，线程操作这些变量副本，并不直接写主存。主存中保存所有变量的真身(master copy)，也就是说线程操作的是栈中保存的变量的分身，当然分身也可以通过一些操作覆盖真身，后面会讲到。在真身(主存中的变量)和分身(栈中的变量)的相互复制之中，需要满足一些规则。

主存中的每一个变量都有一个锁(lock)，线程可以尝试获取变量的锁。

在本章中，每一个线程可以执行如下操作use, assign, load, store, lock, unlock。主存子系统(main memeory subsystem)可以执行read, write, lock, unlock这些操作。以上的每一个操作都是原子的。

use、assign操作与线程执行引擎和线程栈紧密相关。lock和unlock操作与线程执行引擎和主存紧密相关。但是对于数据在主存和栈两个地方的来回传递，是松耦合的。当一个数据要从主存中拷贝到栈中时，两个操作必须发生：主存必须执行一个read操作，在之后的某一个时刻，对应的一个load操作必须发生在栈上。当一个数据要从栈拷贝到主存时，同样必须发生两个操作：栈上必须执行一个store操作，之后的某一个时刻，主存必须执行一个write操作。主存和栈区每一条指令执行时，都会有一个取指的运输时间(transit time，和CPU流水线结构相关)，并且不同的数据处理流(transaction)的这个时间也会不同。因此针对某一个线程对不同变量的操作，操作的初始时间构成的顺序，不同线程看到的也会有所不同。但是，不管是主存代替线程执行还是线程本身自己执行，执行的操作顺序(operation order)一定是一样的。

>**Note:** 这里我理解的是，比如对于线程T1，操作两个变量a和b，顺序是先a后b，但是操作a的变量指令取指运送时间长，后面的b反而短。当这种现象发生时，线程T1依然以为是先a后b，但是对于另一个线程T2来看，好像是b先于a发生了操作

每一个线程从程序的语义角度来看，都可以看成是由一系列的use, assign, lock, unlock构成的。在Java虚拟机规范中，线程还需要附加执行一些load, store, read和write操作，从而去满足一些条件和限制，这个后面会介绍到。如果虚拟机的实现者和程序的编写者遵循了这些限制，那么JVM规范可以保证，数据可以安全可信赖的在主存和栈中来回交换赋值。这些JVM规范是良设计的，他们被设计的足够严谨来保证数据可信赖，同时又留有余地给硬件和软件的设计者，可以利用注册、队列、缓存来充分的提供程序运行速度和吞吐量。

下面是各个操作的详细定义

* 线程执行的use操作，可以将栈中的变量传递给线程的执行引擎。每当一个线程执行一个需要用到变量去计算的虚指令(虚拟机指令)时，use操作都会被执行
* 线程执行的assign操作，可以将线程执行引擎中的变量传递给栈。每当一个线程执行一个需要复制给变量的虚指令时，assign操作都会被执行
* 主存执行的read操作，可以将主存中的变量内容发射(transmit)给线程栈，之后load操作会用到read发射出的变量值。(很多中文书这里都讲错了，read操作并没有完成主存到栈，数据复制的全过程，就像从北京到上海，read只是离开了北京而已，load才是到了上海。道理很简单，因为对于计算机系统而言，没有指令能做到主存到主存(栈也是在memory中)的转移，都需要一次主存到寄存器，再从寄存器到主存的过程)
* 线程执行的load操作，将read操作发射的变量保存到栈中
* 线程执行的store操作，将栈中的变量内容发射出去，目标是主存
* 主存执行的write操作，将store操作发射出来的变量值写入到主存
* lock操作(需要线程和主存紧密协作，同步完成)，线程尝试获得某一个锁
* unlock操作(需要线程和主存紧密协作，同步完成)，线程尝试释放某一个锁

因此，可以看出，线程和变量的交互是由一系列的use, assign, load, store构成的，主存来配合为每一个load操作执行一个read操作，为每一个store操作执行一个write操作。线程和锁的交互就是一系列的lock和unlock。所有这些对变量的操作和锁的操作，统一构成了线程展现给外界的所有行为。

## 8.2 执行顺序和一致性

执行顺序的规则限制了可能会发生的事件集合的顺序。在各个操作关系中，有四个基本规则限制，他们是

* 每一个线程执行的操作(action)都是有序的。也就是说，对于一个线程执行的任意两个操作来说，一定有一个操作先于另一个操作发生
* 对于主存而言，对每一个相同变量执行的操作都是有序的。也就是说，对同一个变量主存执行的任意两个操作，一定有一个操作先于另一个操作发生
* 对于主存而言，对每一个锁的操作都是有序的。也就是说，对于同一个锁，主存执行的任意两个操作，一定有一个操作先于另一个操作发生
* 不允许有连续的相同操作(It is not permitted for an action to follow itself)

最后一个限制看上去有一点多余，但是实际上是有必要单独并明确声明的。如果没有最后一条规则，那么就有可能在多线程的场景下，存在一组操作集合，他们先后顺序满足前三条规则，但是存在连续的相同action。这里单独列出这条性质，可以避免出现这种情况。

线程间不会互相直接交互，他们需要通过主存来实现。线程的操作集合和主存的操作集合，两者需要满足下面三个限制

* 一组线程和主存构成的lock和unlock必须是成对的
* 线程的load操作必须和主存的read操作一一对应，并且load操作必须在read操作之后
* 线程的store操作必须和主存的write操作一一对应，并且store操作必须在write操作之前

下面的大部分规则都是在以上的规则基础上，进一步限制了一些操作的执行顺序。这些规则，可能限制了某一个操作必须发生在某些操作之前或者之后。需要注意的是，这些操作顺序是可推演的: 如果操作A必须先于操作B，操作B必须先于操作C，那么操作A一定是先于操作C的。程序员需要知道的是，操作的顺序只受限于规则限制。如果没有规则、或者没有规则的某种组合，来限制操作A和操作B的顺序，那么JVM的实现者对于操作A和操作B的顺序是随意的，某一个JVM实现版本可能是A先于B，另一个JVM实现版本可能是B先于A，甚至B和A并发/并行，这些都是合法的。这种自由对于实现高性能JVM是非常重要的。当然，一个JVM的实现版本也没有要求必须要充分利用这种自由。

在下面的规则限制中，短语“B必须介入A和C”，表示操作B必须在A之后、C之前发生。

## 8.3 变量的规则

我们定义T表示一个线程，V表示一个变量。T执行过程中对V有如下限制

* 根据标准执行模型(standard execution model)，T对V执行use或者assign操作，当且仅当T在执行到需要用到use或者assign指令的代码时，才允许执行use和assign操作。举个例子，当线程执行到一个这样的时刻，此时V作为`+`操作符的操作数时，一个针对V的use操作将会发生。又比如V作为`=`操作符的做操作数时，一个针对V的assign操作将会发生。给定一个线程，所有的use和assign操作，必须严格按照程序定义的顺序来依序发生。如果下面的限制禁止线程选择use作为下一个操作，那么T可能有必要先选择执行一个load操作来推动程序继续往下发展。
* T对V执行的store必须介于T对V的assign和随后的T对V的load(非正式：线程不允许漏掉最近的一次assign操作)
* T对V执行的assign必须介于T对V的load/store和随后的T对V的store(非正式：线程不允许随意将栈区变量值同步到主存中)
* 在T创建成功后，对V执行use/store之前必须先执行assign/load(非正式：线程的栈区初始状态是空的)
* 在V创建成功后，每一个线程对V执行use/store之前必须先执行assign/load(非正式：任何一个变量创建成功后，初始时总是只存在于主存中，并不存在于栈中)

只要满足了8.3, 8.6, 8.7中的规则限制，任意线程对任意变量在任意时刻都可以执行load和store操作，这完全看JVM实现者的心情。

主存执行的read和write操作也有规则限制，如下

* T对V执行的load操作，都必须有一个对应的read，并且load写入栈的值必须是对应read发射出的值
* T对V执行的store操作，都必须有一个对应的write操作，并且write写入主存的值必须是对应的store发射出的值
* 假设A是T对V执行的load/store，P是主存对V执行的read/write。相似的，假设B是T(同一个线程)对V执行的load/store，Q是主存对V执行的read/write。如果A先于B发生，那么P一定先于Q发生(非正式：针对某一个变量，主存执行操作的先后顺序，和这些操作匹配的线程操作执行顺序是相同的)

注意，上面的最后一条规则，仅适用于某一个线程针对同一个变量的操作而言。另外，如果这个变量是volatile修饰，那么它还会有更严格的限制，8.7会讲到这方面的内容。

## 8.4 double、long型变量的非原子性

如果一个double/long型变量(长整形,64bit)没有声明为volatile，那么针对这个变量的load,store,read,write操作都实现为好像是两个32bit变量的对应操作。在满足规则限制下，对V执行上面每一个的操作，都会执行两遍，每次操作32bit。需要说明的是，将64bit变量底层编码为两个32bit的变量体(32bit quantities)，并且这两个32bit的先后顺序都不是Java语言规范定义的。

这需要注意，因为对double/long变量的read/write操作，被分裂成了两个32bit变量的read/write操作，并且这两个操作之间有时间间隙，别的操作在规则允许的条件下也可能会插入到这两个操作之间。更进一步，如果两个线程分别将两个不同的值并发复制给同一个非volatile的double/long变量，那么随后的use操作可能取到的数与初始化的两个数都不相等，可能是两个数的32bit的某一种混合，这需要视JVM的实现细节来决定。

是否需要将64bit变量的上述操作实现为原子操作，关于这一点并没有要求。但是，我们鼓励JVM的开发者实现成原子操作。分割变量bit值，主要是因为现在流行的32位的微处理器架构并不能很好的支持64位变量的内存原子操作。如果我们能够将任意变量的内存操作都实现为原子操作，那无疑是非常便于开发者编写易于理解的代码，现在关于这一点的定义复杂化，还是处于对当前硬件架构的一种妥协让步。在将来，64Bit变量的非原子性操作也许有可能会被改善。但是，此时程序员还是非常有必要认识到对于共享的double/long变量(多线程访问)操作，我们需要进行多线程同步。

## 8.5 锁的规则

我们定义T表示一个线程，L表示一个锁。T执行过程中对L的操作有如下限制

* T可以持有L，当且仅当除了T之外的所有线程集合*S*之前对L执行的unlock操作数和lock操作数相等(非正式：在任一时刻，一个锁只能被一个线程持有，进一步，一个已经持有L的T，还可以继续多次进行lock操作，增加加锁的深度。只有当执行了对应深度的unlock操作之后，这个线程才算真正释放了锁L)
* T可以对L执行unlock，当且仅当T对L执行的unlock次数小于lock次数(非正式：T不允许在未持有L的情况下执行unlock操作)

对于锁L，所有线程对L的lock/unlock组成一个串行的序列，并且这个序列和每一个线程对L的lock/unlock是一致的。

## 8.6 锁与变量交互的规则

定义T代表任意线程，V表示任意变量，L表示任意锁。一下是T操作V和L的限制

* `assign -> store -> write -> unlock` T对V的store必须介于T对V的assign和T对L的unlock；此外，主存对应store的write操作必须先于unlock(非正式：如果线程要执行一次unlock操作，那么必须将之前已经assign过的数据同步写回主存中) ，可以这么理解，如果你已经通过加锁实现同步来完成某些变量的更新，那么针对这些assign过的更新变量理应同步会主存，因为不会有线程跟你竞争
* `lock -> /read -> assign/load -> store/use` 对V的assign/load必须介于T对L的lock和T对V的use/store之间；此外，如果T对V的操作是load，那么与之对应的read操作必须跟在lock之后。(非正式：一个lock操作就好像要求线程在assign或者load之后，必须将栈中的变量flush掉。注意，针对lock，并没有要求flush的变量必须写回到主存) 

## 8.7 volatile变量的规则

如果一个变量被声明为volatile类型，那么线程会被追加要求满足如下的规则限制，其中T表示一个线程，V和W表示一个volatile变量

* `read -> load -next> use` T允许对V执行use操作，当且仅当T对V的上一个操作是load，并且，T允许对V执行load操作，当且仅当T对V的下一个操作是use。在这种情形下，我们称use操作与read操作是关联(associated)的
* `assign -next> store -> write` T允许对V执行store操作，当且仅当T对V的上一个操作是assign，并且，T允许对V执行assign操作，当且仅当T对V的下一个操作是store。在这种情形下，我们称assign操作与write操作是关联的
* 用A表示T对V执行的use/assign，F表示与A对应的load/store，P表示与F对应的read/write。相似的，我们有针对T之于变量W的定义B、G、Q。如果A先于B发生，那么P一定先于Q发生(非正式：主存对volatile变量执行的对应操作read/write顺序，与线程执行操作load/store的顺序是一致的)

> 在JSR-133(Java Specification Requests，Java规范提案)中，对volatile的要求作了进一步的限制，要求在对volatile变量进行读写时，其前后的指令在一定条件下不允许重排序(包含编译重排序和CPU重排序)，条件如下
>
> * 如果是一条对volatile变量进行写操作的代码，那么该代码前面的任何代码都不能与这个写操作交换顺序。
> * 如果是一条对volatile变量进行读取的操作代码，那么这个代码后面的所有操作都不能与这一条读指令交换位置
> 
> 可以这么理解，对volatile变量赋值之前的代码happen-before这个赋值操作。对volatile变量读取的代码happen-before之后的代码

## 8.8 前置的预测型store操作

如果一个变量没有声明为volatile类型，那么在8.7节定义的规则限制里关于store条件将被放宽松，store操作发生的时间允许早于变量为volatile时所要求的时间点。也就是说，在volatile时，store必须发生在assign之后，但是对于非volatile变量，允许store发生在assign之前，也就是说非volatile变量即使更新了，也不一定会立刻发射给主存。放宽限制的目的是允许编译器进行一些指令重排序(code rearrangement，译者觉得这里的指令重排序主要强调的是store前置)从而优化代码，这样的宽松能够使那些做了很好的同步工作的程序保持逻辑语义不变，但是对于哪些并没有做好充分同步工作的程序，他们的内存操作可能会发生混乱，导致数据错误

> 如何理解上面最后的一段话呢？我的理解是在有lock的情况下，多线程程序只有一个线程能够持有锁，在lock和unlock之间，即使有重排序，在unlock之前也会完成同步更新回主存(包含write操作)，从而实现逻辑语义不变，见8.6第一条限制，unlock前所有assigned变量都会完成同步写回主存。然而，如果没有做同步，即lock/unlock，那么一个线程的store/assign更新后，变量最新值也许并没有同步回主存(因为不知道write会发生在什么时刻)，此时其他线程从主存来读取这个变量时，依然是旧的值，从而导致数据错误

让我们假设如果T对V的store操作，发生在T对V的assign之后，就像上一节规定的那样，并且T对V的下一个操作不是assign/load。那么这个store操作将会把assign操作更新的最新变量值发射给主存。重新回到本节讨论的问题点，我们确实允许非volatile变量的store早于assign发生，只要满足以下条件

* store虽然先发生了，但assign随后一定会发生(注意，这里强调了一定，并不是应该。即，如果store之后，assign前，出现了异常，导致assign没有发生，这样的情况store不能前置)
* 前置的store和assign之间，不能有lock操作
* 前置的store和assign之间，不能对V进行load(这条限制译者没太看明白，如果哪位了解，希望能跟进)
* 前置的store和assign之间，不能有对V的其他store操作
* 前置的store发射给主存的V的值，必须是随后的assign操作将会更新的目标值

上述的最后一条限制非常重要，正是因为这一条限制，我们才称这样的前置store操作为预测型store操作，因为他已经提前知道了随后assign操作计算出来的目标值。这种提前预判是可能的，在现代的工业实践中，优化后的代码将会提前计算这样的值，前提是这样的计算不会产生副作用或者导致异常。然后提前将变量值发射回主存(比如，在进入一个循环之前，提前计划更新主存里的这个变量)，并且将提前计算好的变量值保存在寄存器中，在接下来的循环中会被用到。

## 8.9 讨论

任何锁与变量之间的关联都是非常纯粹的基于上面的限制。任何lock操作从概念上来看都是将栈区的变量flush掉，任何unlock操作会将所有assign过的变量写回到主存中。对象和类，他们和锁的关联也是非常纯粹的基于上面的限制。比如，在一些应用中，经常会看到对一个对象加锁，然后读取这个实例的变量值，一个简单的synchronized方法就可以很方便的做到这一点。在另外一些应用中，也有单独顶一个锁变量，然后对这个锁变量加锁，从而线程安全的读取某些海量数据集。

如果一个线程要用到一个多线程共享的变量，那么这个线程应该先持有一个锁，执行一些操作*S*，随后释放这个锁。在这种情境下，操作*S*用到的变量将是从主存同步过来的最新值，如果*S*里存在assign操作，那么assign操作后的值会在unlock前同步写回到主存中。通过锁的互斥规则，我们能够保证多线程共享变量可以安全的在主存和线程栈中来回赋值计算。

volatile的规则要求一个线程对volatile变量的use/assign操作都会触发一次主存的读/写访问，并且主存的读写访问顺序，和线程代码执行逻辑的语义顺序是一致的(8.7的第三条规则)。需要注意的是，如果是非volatile变量，这样的顺序一致性不能够得到保证(线程中的变量操作会有重排序发生)。

## 8.10 例子：交换函数的多种可能性

考虑一个Sample类，类中有变量a,b，函数hither和yon，定义如下

```
class Sample {
    int a = 1, b = 2;
    void hither() {
    	a = b;
    }
    void yon() 
    	b = a;
    }
}
```

现在假设有两个线程被创建，一个线程调用hither，另一个线程调用yon。思考以下两个问题，这两个函数的对应操作集合是什么呢？这些操作执行的顺序有什么限制么？

首先我们考虑调用hither的线程。根据上面提到的规则限制，这个线程一定会对b执行use操作，并且后续会对a执行assign操作。这是执行hither函数的最基本操作。

更进一步，线程对变量b的第一个操作不可能是use，应该是assign或者load。assign是不可能的，因为hither函数代码不会执行assign命令，因此在对b的use之前，会对b执行load操作，那么在load之前，还会再追加read操作。

在对a执行assign后，后续的store操作实际上是可选的，可能会同步回主存，也可能不会。如果我们的程序会进行同步写回，那么在store之后还会追加一个对a的write操作。

对于另一个执行yon的线程来讲，这两个线程的执行上下文是相似的，只是变量a和b互换了一下而已。那么这两个函数的对应操作集合列举如下

![GitHub set up](http://docs.oracle.com/javase/specs/jvms/se6/html/Threads.doc.anc.gif)

图中的箭头从A指向B，表示操作A先于操作B发生

对于主存而言，这些操作执行的顺序可能有哪些组合呢？这里唯一的限制是，对变量a的write操作不可能先于read，对变量b的write操作不可能先于read，这样才能保证上面的因果关系箭头不会形成一个闭环。如果存在闭环，那么隐含的语义是一个操作需要先于自己发生，这是不可能的。另外，假设线程的store和write操作需要发生，下面是主存可能的三种逻辑执行顺序。我们定义ha/hb表示执行hither线程的栈区变量a和b的值，ya/yb表示执行yon的线程，ma/mb主存中变量a/b的值。初始时，`ma = 1, mb = 2`。那么三种可能的执行顺序以及他们最终的结果值总结如下

* `write a -> read a, read b -> write b (then ha=2, hb=2, ma=2, mb=2, ya=2, yb=2)`
* `read a -> write a, write b -> read b (then ha=1, hb=1, ma=1, mb=1, ya=1, yb=1)`
* `read a -> write a, read b -> write b (then ha=2, hb=2, ma=2, mb=1, ya=1, yb=1)`

因此，在主存中最后的可能结果是a拷贝到b，或者b拷贝到a，或者a和b的值互换；另一方面，线程栈区的变量值与主存中的值可能相等，也可能不相等。我们不能假设并断言某一种结果比另外的结果更可能发生，因为这个程序两个线程执行后的结果，仅和时间相关。

当然，其他的虚拟机实现版本可能选择不会给函数追加store和write操作，或者只追加store和write两者之一，那么分析结果也会有所不同，这里不再继续赘述。

现在假设我们将上面的函数修改为同步的版本

```
class SynchSample {
    int a = 1, b = 2;
    synchronized void hither() {
    	a = b;
    }
    synchronized void yon() 
    	b = a;
    }
}
```

让我们再一次考虑执行hither函数的线程。根据上面提到的规则限制，在线程执行hither函数之前，一定会执行lock操作(操作目标是SynchSample类的对象实例)。后续的操作是对b的use和对a的assign。最后，在函数执行完hither之后，会对同一个SynchSample类对象执行unlock操作。以上是线程执行hither函数的最基本操作步骤。

在此之前，需要对b执行load，根据规则，在load之前，主存需要对b执行read。因为load发生在lock之后，与load对应的read也同样需要发生在lock之后。

因为对a的assign之后有unlock操作，那么对a的store操作是强制需要的，作为结果，对a的write操作也是必要的。并且，对a的write操作必须发生在unlock之前。

对于另一个执行yon的线程来讲，这两个线程的执行上下文是相似的，只是变量a和b互换了一下而已。那么这两个函数的对应操作集合列举如下

![GitHub set up](http://docs.oracle.com/javase/specs/jvms/se6/html/Threads.doc.anc1.gif)

lock和unlock对主存的操作执行顺序提出了更进一步的限制。一个线程的lock操作，不能发生在另一个线程的lock和unlock之间。更进一步，与上面非同步的代码版本不同，unlock操作要求store和write是必须的，因此也去掉了中框号可选标示。同步版本，只有两个可能的发生序列

* `write a -> read a, read b -> write b (then ha=2, hb=2, ma=2, mb=2, ya=2, yb=2)`
* `read a -> write a, write b -> read b (then ha=1, hb=1, ma=1, mb=1, ya=1, yb=1)`

结果具体是这两个序列的哪一个只与他们的发生时间有关，此外，我们可以看到，不管是两者中的哪一个，两个线程栈区的变量值都最终和主存达成了一致。

## 8.11 例子：乱序写

这一节的例子与上一节的例子是类似的，只是函数变成了一个重新设置变量的函数和一个只读取两个变量的函数。函数定义如下

```
class Simple {
    int a = 1, b = 2;
    void to() {
    	a = 3;
    	b = 4;
    }
    void fro() 
    	System.out.println("a= " + a + ", b=" + b);
    }
}
```

现在假设有两个线程被创建，一个线程调用to，另一个线程调用fro。思考以下两个问题，这两个函数的对应操作集合是什么呢？这些操作执行的顺序有什么限制么？

让我们考虑一个调用to函数的线程。根据上面提到的规则限制，这个线程必须对a执行assign，随后对b执行assign。这是调用to函数的最基本操作。因为函数没有同步，因此对于一个虚拟机的实现版本而言，是否后续追加store操作来写回主存是没有要求的。因此，对于调用fro函数的线程，a和b的结果值没有相关性，a的值可能是1或者3，b的值可能是2或者4。

现在我们保持fro不变，对to增加同步

```
class SynchSimple {
    int a = 1, b = 2;
    synchronized void to() {
    	a = 3;
    	b = 4;
    }
    void fro() 
    	System.out.println("a= " + a + ", b=" + b);
    }
}
```

在这种情形下，函数to在执行unlock之前，将会调用store强制将assigned变量写回主存。

函数fro需要从主存load变量a,b，并且随后use变量a和b(先a后b的顺序)。

可能的操作集合如下

![GitHub set up](http://docs.oracle.com/javase/specs/jvms/se6/html/Threads.doc.anc2.gif)

对主存而言，这些操作的可能发生顺序是什么？注意，并没有规则要求对a的write要先于对b的write，也不要求对a的read要先于对b的read。另外，由于函数to同步，fro不同步，所以我们不能防止read操作发生在lock和unlock之间。(所以，这里的关键点是，即使我们将一个函数声明为synchronized，也不能保证这个函数的执行时原子的，就执行序列而言，非同步的函数操作也可能发生在lock和unlock之间)

作为结果，fro函数执行后，a可能是1或者3，b可能是2或者4。因此，fro函数观测到`a=1, b=4`也是可能的。也就是说，虽然to函数先执行了对a的assign，然后是对b的assign，但是对应的store函数顺序是不确定的，其他的线程观测到的store操作顺序可能是先b后a，从而导致了结果`a=1, b=4`。

最后，假设to和fro都是同步的

```
class SynchSynchSimple {
    int a = 1, b = 2;
    synchronized void to() {
    	a = 3;
    	b = 4;
    }
    synchronized void fro() 
    	System.out.println("a=" + a + ", b=" + b);
    }
}
```

在这种情况下，fro函数的操作不会和to函数的操作交织在一起，所以fro只会打印`a=1, b=2`或者`a=3, b=4`

## 8.12 线程

线程是通过类Thread、ThreadGroup来创建和管理的。创建一个Thread对象从而创建了一个线程，并且这也是唯一的创建线程的方式。当一个线程被创建后，这个线程此时并不处在活跃的运行状态。线程只有在调用了start方法后，才会开始运行。

## 8.13 锁和同步

每一个对象都有一个锁与之关联。Java编程语言并没有提供专门的指令来实现上文提到的lock和unlock操作，而是使用一种潜在的高级别构造形式来实现lock/unlock元语(与之对应的，Java虚拟机提供*monitorenter*和*monitorexit*指令来实现lock/unlock)

synchronized关键字能引用到一个对象，并尝试对这个对象进行lock加锁，并且在lock操作成功完成之前，都不会执行后面的指令，此时线程是阻塞的。通常，对于通过synchronized关键词进行同步保护的代码片段，不论是正常执行完，还是发生了异常，Java语言的编译器能够保证以下的执行顺序 `lock(monitorenter) -> statement body -> unlock(monitorexit)`

synchronized函数在被唤醒(invoke)时能够执行lock操作，只有在lock成功之后，才会执行这个函数体。如果这个函数是对象内部函数，synchronized锁的是函数所在对象关联的锁，即Object Instance(加锁的对象一般我们也称为关键字*this*)。如果这个方法是静态(static)的，那么synchronized锁的则是class对象。无论是哪一种，无论是正常执行完还是发生了异常，与lock对应的unlock操作都会在函数执行完毕时被执行，从而释放锁。

最好的实践是，如果某个线程曾对一个对象执行过assign，并且其他线程也曾对这个对象执行过use/assign，那么所有访问这个对象的函数和代码段都应该使用synchronized来进行同步保护。

虽然Java编译器保证了lock/unlock的正确使用，但是并非所有提交给JVM的代码都能够做到这一点(比如有人直接写机器级代码...)，所以JVM的实现版本**不**强制要求满足以下两个规则

* 不管函数是正常结束，还是发生了异常。在程序的执行过程中，所有对L的lock操作数，必须和对L的unlock数相等。
* 函数中不能存在这样一个点P，从函数开始到P中所有对L的unlock操作数多于从函数开始到P中所有对L的lock操作数。

上面的规则用非正式的语言描述就是，在函数中，对L的所有unlock都能在前面找到一个唯一对L的lock与之匹配。

注意，在执行一个synchronized函数时，JVM自动lock/unlock操作只发生在函数的执行过程中。

## 8.14 线程等待集合，通知

每一个对象，除了有一个锁与之关联，还有一个线程等待集合(a wait set of threads)与之关联。当一个对象被创建之后，这个线程等待集合是空的。

线程等待集合是为对象的`wait, notify, notifyAll`这三个函数服务的。这些函数与线程的调度机制相关。

对于线程T来讲，只有当T获得了对象的锁之后，才可以调用这个对象的`wait`函数。现在假设T已经对某个对象执行了N次lock操作(这里的N此lock操作是指那些没有被unlock抵消掉的，当前正在生效的lock操作)。如果此时调用这个对象的`wait`，那么就会把当前线程T加入这个对象的线程等待集合中，取消掉这个线程的被CPU调度资格，然后执行N次unlock操作从而释放这个对象。这里的N次unlock并不像synchronized结束时的unlock那样真的要去释放锁，其目的是因为当前某个条件不满足而不得不去等待这个条件发生，所以这才释放锁，当条件满足时，这些锁将会被重新全部加上。线程T将保持这种休眠的状态，直到下面事件中任意一个发生

* 其他线程调用了这个对象的`notify`，此时会随机选择这个对象的线程等待集合中的某一个线程来唤醒，线程T碰巧被选中了。
* 其他线程调用了这个对象的`notifyAll`，此时这个对象的线程等待集合中所有线程都被唤醒，都进入可以被CPU调度的状态，一个线程将通过竞争获得锁从而获得了执行的权力，其他的线程则进入阻塞状态。注意这里的阻塞状态与wait时的等待并不同，wait有一个等待集合来容纳这些线程，相当于有一个休息室，这些线程正在里面休息。而阻塞的线程都在努力争取可能会被释放的锁，他们并不在等待集合休息室中休息。
* 如果线程T在调用`wait`时声明了等待的时间，当确实已经逝去了这些事件后，线程T将会离开线程等待集合，并尝试获得这个对象的锁。

在上面三个事件任意一个发生之后，线程T将会从对象的线程等待集合中移除，并且重新获得被CPU调度的权力。T将重新和其他线程像平常那样尝试获得对象的锁，一旦成功获得对象的锁，T将对这个对象再执行N-1次lock操作(获得对象锁的时候，加了一次锁，这里再执行N-1次，从而实现对这个对象加锁N次，这样保证了和之前wait时的上下文相同)，然后程序执行重新指向了之前程序wait操作的下一条指令。

只有当当前线程T获得了某个对象的锁，才可以调用这个对象的`notify`函数，否则将会抛出IllegalMonitorStateException异常。调用`notify`之后，如果这个对象的线程等待集合不空，那么就会随机挑选一个线程，从集合中移除，使之重新获得被调度资格。值得一提的是，这个重新获得调度权的线程并不会立刻被调度，它需要等当前线程T执行完`notify`之后所有在加锁区域的指令，然后T释放锁，再然后这个线程才会有机会真正被调度。

只有当当前线程T获得了某个对象的锁，才可以调用这个对象的`notifyAll`函数，否则将会抛出IllegalMonitorStateException异常。调用`notifyAll`之后，所有这个对象的线程等待集合中的线程都会被移除，并去竞争获得对象锁，当然以上的去竞争锁需要等到线程T执行完notifyAll之后所有在同步区代码并释放锁之后方可。

